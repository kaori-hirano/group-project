---
title: "Extended Outline"
author: "Group 4: Kaori Hirano, Alicia Nguyen, James Xia"
date: "07/27/23"
format: pdf
execute: 
  error: true
  message: false
  warning: false
  eval: true
---
```{r load-packages, echo = FALSE}
library(readr)
library(readxl)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(dplyr))
library(patchwork)
suppressPackageStartupMessages(library(glmnet)) # for ridge, LASSO
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(caret))
library(Matrix)
library(broom)
library(tree)
library(tibble)
```
note - have we updated the codebook, if not we need to
# Introduction and Data
## Topic & Research Question
The introduction provides motivation and context for your research. Describe your topic (citing sources) and provide a concise, clear statement of your research question and expectations.

## Data
Then identify the source of the data, when and how it was collected, the cases, a general description of relevant variables. You should describe any data wrangling that you have done, any variables you have changed from the original data set, the data tidying you had to do, and what variables you plan to include in any models you will do, and why.

```{r merge-datasets}
wh_2023 <- read_excel("/cloud/project/data/wh_2023.xls")
load("/cloud/project/data/vdemdata-master/data/vdem.RData")
## load the original datasets

need_vdem=c("country_name","year","v2csreprss",'e_civil_war',"e_pt_coup",
"v2x_partipdem","e_peaveduc","v2x_corr","v2x_cspart","v2xcs_ccsi", 'e_regionpol')
vdem_use=vdem[,need_vdem]
need_wh=c("Country name","year","Social support","Freedom to make life choices","Generosity","Life Ladder","Healthy life expectancy at birth","Log GDP per capita")
wh_use=wh_2023[,need_wh]
## only select the portions where we need to need for our project

colnames(wh_use)[1] <- "country_name"
## change col name for eaier merge

total=merge(vdem_use, wh_use, by = c("country_name", "year"),
            all.x=TRUE,
            all.y=TRUE) 
##merge data
```
 
  In this part, nearly all variables in wh_2023 are selected because we do not have sufficient background to interpret necessary relationships. We do the same for vdem_use dataset. However, incorporating all datasets is not possible. Thus, we incorporate all the quantified data variables associated with political events and possible social activities. Thus the final incorporated is a comprehensive dataset based on full analyses of given codebooks.  
  
```{r clean-data}
#| message: false
#| warning: false
# note -- why are we including this? we are only using 2019

# Select the time range from 2012 (inclusive) and later only 
total_2019 <- total[total$year == 2019,]

#replace missing values in each numeric column with median value of column
# NEED TO TALK ABOUT WHY WE DID THIS
total_2019 <- total_2019 %>% mutate(across(where(is.numeric),~replace_na(.,median(.,na.rm=TRUE))))

# removing coup and civil war because it was all NA or 0 for 2019
total_2019 <- total_2019 %>% select(-one_of("civil_war", "coup"))

# changes names to shorter and easier to type/remember forms
cs <- total_2019 %>% rename(csrepress = v2csreprss, civil_war = e_civil_war, 
                           coup = e_pt_coup,
                           edu = e_peaveduc, corr = v2x_corr, 
                           cspart = v2x_cspart, 
                           cs_index = v2xcs_ccsi, 
                           social_support = 'Social support', 
                           choices = 'Freedom to make life choices', 
                           gen = Generosity,
                           region = e_regionpol,
                           happ = "Life Ladder", 
                           lifee = "Healthy life expectancy at birth", 
                           gdp = "Log GDP per capita")
```
For the sake of completeness of the data, no additional changes are made. For further analysis, the NA data are omitted. 
```{r import-data, echo=FALSE}
# import data
cs_full <- readRDS("/cloud/project/data/civil_society")
# get only 2019 and remove civil war and coup because there are none in 2019
cs <- cs_full %>% subset(year == 2019) %>% select(-one_of("civil_war", "coup")) %>% drop_na()
```

```{r region-tidying}
# sets as factor
cs$region <- as.factor(cs$region)

# looks for region with most observations to set as base level
cs$region <- relevel(cs$region, ref = 4)

# creates new data frame for tree plotting with regions as numbers still
cs_num <- cs

# recodes region to be the region it represents rather than a number value/code
levels(cs$region) <- c('SubSaharanAfrica',
                    'EasternEurope_PostSovietUnion', 
                    'LatinAmerica',
                    'NorthAfrica_MiddleEast',
                    'WesternEurope_NorthAmerica',
                    'EasternAsia',
                    'SouthEasternAsia',
                    'SouthernAsia',
                    'ThePacific',
                    'TheCarribean')
```
The names of used variables are shortened for further analysis. The year 2019 is chosen as our focus specifically because we want to focus on the change before and after pandemic year. 2019 is the most recent data set we have that is not reflecting social changes due to COVID-19. This might be more representative of the social conditions before the pandemic year and thus help us answer our main research question of how civil participation has changed before and after the pandemic. 
  
The introduction provides motivation and context for your research. Describe your topic (citing sources) and provide a concise, clear statement of your research question and expectations.

Then identify the source of the data, when and how it was collected, the cases, a general description of relevant variables. You should describe any data wrangling that you have done, any variables you have changed from the original data set, the data tidying you had to do, and what variables you plan to include in any models you will do, and why.

# Methodology
The methodology section should include visualizations and summary statistics relevant to your research question. You should also justify the choice of method(s) used to answer your research question.

note: do a visualization between region and social support, as well as one between civil society participation and participation in democracy? or even civil society index as well. actually region and like as many things as possible would be great - must understand the similarities between regions in the results so any way to compare the means of them in various important predictors or like any predictors would be swell

```{r variable-summary}
#| warning: false
#| message: false
# note is there a way to make this look nicer than just printing the output?
# idk if that's something they care about (you'd probably know better from your last class)
# also, are all of these relevant? and if so we should say why
# it seems like they're really just looking for us to justify everything we do
summary(cs)
```

## Data Changes for Model Choice
We can see that the scale of the variables used as predictors vary widely. This is why it is ideal to scale the data while doing Lasso and Ridge regression. We do not have to scale data in random forest model. 

## Model Choice 
Our research question regards which predictors are the most important in predicting civil society participation across countries. We further edited this question to focus on 2019, specifically, to make our analysis more feasible. Our outcome variable is a numeric. We chose to used regression to identify the most important predictors of civil society participation in 2019. Lasso will eliminate non-influential variables, while ridge will shrink them to a smaller value. The optimal lambda will be determined using 10-fold cross validation. We also used tree models to further understand the most important variables. We initially chose a single decision tree to give a base understanding of the main predictors visually. However, because single decision trees are not the best in terms of prediction compared to other tree models, we also used a random forests model. Random forests allows us to see a relative ranking of the most important variables and their relationships to each other in our model. The hyperparameters (terminal nodes for the single decision tree and number of variables considered for random forests) will be chosen by 5-fold cross validation. If the top most important variables produced by our machine learning models are consistent with our hypothesis, and the model works well when applied to the test set, this means that the model results are relatively consistent with our hypothesis. We also want to see the specific relationship, negative or positive, and the magnitude of the relationship between our outcome variable and each of the predictor variable to help shed light on our research question. 

```{r variable-visualization}
#| warning: false
#| message: false

## note do we want an lm line for non linear? or smooth w/ no method?

# A scatter plot to show the relationship between civil society participation 
# and social support
viz1 <- ggplot(cs, aes(x=social_support, 
               y = v2x_partipdem)) + 
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Social support",
       y = "Participation in democracy",
title = "Relationship between Participation in Civil society and Social Support",
color = "Regions")

# A scatterplot to show the relationship between civil society participation 
# and participation in democracy
viz2 <- ggplot(cs, aes(x=cspart, 
               y = v2x_partipdem)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Civil society participation",
       y = "Participation in democracy",
title = "Relationship between Participation in Civil society and in democracy")

# A scatterplot to show the relationship between civil society participation 
# and civil society index
viz3 <- ggplot(cs, aes(x=cs_index, 
               y = cspart)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Civil society index",
       y = "Civil society participation",
       title = "Relationship between Civil Society Participation and Index")

viz1 + viz2 + viz3 #+ 
  # geom_text("Relationship between civil society participation and predictors")
```

```{r visualize-region-diff}
# A box plot to show the relationship between region and 
# Civil Society Participation

# note: we have the regions listed twice--only need to be on x? or is it a weird graphing thing?

box1 <- ggplot(cs, aes(x=as.factor(region), 
               y = cspart, 
               color = as.factor(region),
               group = as.factor(region))) + 
  geom_boxplot() + 
  labs(x = "Regions",
       y = "Civil society participation by region",
       title = "Civil society participation levels within different regions",
       color = "Regions") + 
  # Rotate x-axis labels by 90 degrees to the left
  theme(axis.text.x = element_text(angle = 45, hjust = 0.9))

# A box plot to show the relationship between region and social support
box2 <- ggplot(cs, aes(x=as.factor(region), 
               y = social_support, 
               color = as.factor(region),
               group = as.factor(region))) + 
  geom_boxplot() + 
  labs(x = "Regions",
       y = str_wrap("Social support by region",
       width = 30),
       title = "Social support levels within different regions",
       color = "Regions") + 
  # Rotate x-axis labels by 90 degrees to the left
  theme(axis.text.x = element_text(angle = 45, hjust = 0.9))

# A box plot to show the relationship between region and Civil Society Index
box3 <- ggplot(cs, aes(x=as.factor(region), 
               y = cs_index, 
               color = as.factor(region),
               group = as.factor(region))) + 
  geom_boxplot() + 
  labs(x = "Regions",
       y = "Civil Society Index by region",
       title = "Civil Society index within different regions",
       color = "Regions") + 
  # Rotate x-axis labels by 90 degrees to the left
  theme(axis.text.x = element_text(angle = 45, hjust = 0.9))

# A box plot to show the relationship between region
# and participation in democracy
box4 <- ggplot(cs, aes(x = as.factor(region), 
               y = v2x_partipdem, 
               color = as.factor(region),
               group = as.factor(region))) + 
  geom_boxplot() + 
  labs(x = "Regions",
       y = "Participation in Democracy by region",
       title = "Participation in Democracy within different regions",
       color = "Regions") + 
  # Rotate x-axis labels by 90 degrees to the left
  theme(axis.text.x = element_text(angle = 45, hjust = 0.9))

box1
box2
box3
box4
```

# Results
## Test Train Split
```{r test-train-split}
# 70, 30 test train split
set.seed(145)
train <- sample(c(TRUE, FALSE), nrow(cs), replace = TRUE, prob=c(.7,.3))
test <- (!train)
val <- test
```

## Regression
```{r matrix-creation}
# create x and y for glmnet
set.seed(129)
x <- model.matrix(cspart ~ csrepress+v2x_partipdem+edu+corr+cs_index+
                           social_support+choices+gen+region+lifee+happ+gdp, 
                  data = cs)[, -1]
y <- cs$cspart
```

### Ridge Regression
```{r ridge-regression}
# set seed for reproducibility
set.seed(129)

# cross validation for best l
cv_r <- cv.glmnet(x[train,], y[train], alpha = 0, 
                  lambda = 10^seq(10, -2, length = 100), scale = TRUE)

# saving optimal lambda
bestlam_r <- cv_r$lambda.min

# calculating MSE
ridge_pred <- predict(cv_r, s = bestlam_r,
newx = x[test, ], scale = TRUE)
ridge_mse <- mean((ridge_pred - y[test])^2)

# fits final ridge model
ridge_mod <- glmnet(x, y, alpha = 0, lambda = bestlam_r, scale = TRUE)

# saves coefficients
coef_r <-coef(ridge_mod)

# prints important coefficients as a table
ridge_feature_estimate <- ridge_mod %>% tidy() %>%
  select(term, estimate)
print(ridge_feature_estimate)
```
### LASSO Regression
```{r lasso-regression}
# set seed for reproducibility
set.seed(18)
 
# cross validation for best l
cv_l <- cv.glmnet(x[train,], y[train], alpha = 1,
lambda = 10^seq(10, -2, length = 100), scale = TRUE)

# saving optimal lambda
bestlam_l <- cv_l$lambda.min

# calculating MSE
lasso_pred <- predict(cv_l, s = bestlam_l,
newx = x[test, ], scale = TRUE)
lasso_mse <- mean((lasso_pred - y[test])^2)

# fits model
lasso_mod <- glmnet(x, y, lambda = bestlam_l, scale = TRUE)

# saves coefficients
coef_l <-coef(lasso_mod)

# prints important coefficients as a table
lasso_feature_estimate <- lasso_mod %>% tidy() %>%
  select(term, estimate)
print(lasso_feature_estimate)
```
# note that the zeroed out levels of the factor are basically the same as the baseline and can be compared against the others at this point.

## Tree Methods
```{r plot-tree, echo=FALSE}
format_tree_labels <- function(labels, levels) {
  sapply(labels, \(x) if(grepl(":", x)) clean_col(x, levels) else clean_lt(x))
} 

# replace letter positions with actual level labels
clean_col <- function(x, levels){
  # split the label into label and levels
  x <- str_split_1(x, ":")
  # make new temp objects for the two components
  var <- x[1]
  levs_ids <- x[2]
  # get levels for correct variable
  levs <- levels[[var]]
  # get level ids for *relevant* levels
  levs_ids <- str_split_1(levs_ids, "") 
  levs_ids <- sapply(levs_ids, \(x) which(letters == x))
  # cut down levs to only the required levels
  levs <- levs[levs_ids]
  # paste everything together and return (immplicitly)
  paste0(var, ": ", paste0(levs, collapse = ", "))
}

# space out labels that include only "<"
clean_lt <- function(x){
  # split on <, then recombine with spaces before and after <
  x <- str_split_1(x, "<")
  paste0(x, collapse = " < ")
}

plot_tree <- function(model){
  require(ggdendro)
  # extract necessary information from tree object so that it is ggplotable
  tree_data <- dendro_data(model)
  # create a data frame with the split *values* which dendro_data() doesn't extract
  frame <- model$frame %>%
    rownames_to_column(var = "split") %>%
    mutate(splits = as.data.frame(splits)) %>% 
    unnest(cols = c(splits)) %>% 
    filter(var != "<leaf>") %>% 
    select(cutleft)
  
  # add the splits information in, which dendro_data() misses
  tree_data$labels <- tree_data$labels %>% 
    bind_cols(frame) %>% 
    mutate(label = paste0(as.character(label), cutleft),
           label = format_tree_labels(label, attr(model, "xlevels")))
      
  ggplot(segment(tree_data)) +
    geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_text(data = label(tree_data), 
              aes(x = x, y = y, label = label), vjust = -0.5, size = 3) +
    geom_text(data = leaf_label(tree_data), 
              aes(x = x, y = y, label = label), vjust = 1.5, size = 2) +
    theme_dendro()
}
```

### Single Decision Tree Model
```{r tree-method}
# sets set for reproducibility
set.seed(247) 

# creates tree on training data
tree_train <- tree(cspart ~ . -year -country_name, cs_num,
    subset = train)

# cross validation of tree
cv_train <- cv.tree(tree_train)

# pruning to 8 nodes per CV results
prune_train <- prune.tree(tree_train, best = 8)

# plots pruned tree
plot_tree(prune_train) + 
  labs(title = "Pruned Tree Plot",
       caption = 'Regions 1, 2, 3, 7, and 9 represent Eastern Europe & Post
       Soviet Union, Latin America, North Africa and Middle East, South Eastern 
       Asia, and the Pacific') + 
  # Center the plot title
  theme(plot.title = element_text(hjust = 0.4))

# gets predictions and test values for mse
tree_pred_tuned <- predict(prune_train, cs_num[test,],
    type = "vector")
y_test <- y[test]

# gets mse for pruned tree
mse <- mean((tree_pred_tuned - y_test)^2)
```
### Random Forest Model
```{r random-forests}
# sets seed for reproducibility
set.seed(286)

# sets cv parameters
train_control <- trainControl(method="cv", number = 5)

# gets grid for mtry
tune_grid <- expand.grid(mtry = 3:12)

# does training
best_forest <- train(cspart ~ . -year -country_name, data = cs[train,], 
                     trControl = train_control, 
                     method="rf", 
                     tuneGrid = tune_grid,
                     verbose = FALSE)

# gets test for y
y_test <- y[test]

# predictions for test set with optimal mtry of 9
rf_cs <- randomForest(cspart ~ . -year -country_name, data = cs[train,], 
                       mtry = 9, importance = TRUE)

yhat_rf <- predict(rf_cs, newdata = cs[test,])

# calculates MSE
rf_mse <- mean((yhat_rf - y_test)^2)

# importances
importances_cs <- importance(rf_cs) %>% 
  as_tibble(rownames = "Variable")

# plot 1
p1_rf <- importances_cs %>% 
  arrange(`%IncMSE`) %>% 
  mutate(Variable = factor(Variable, levels = Variable)) %>% 
  ggplot(aes(x = `%IncMSE`, y = Variable)) +
  geom_col(alpha = 0.5) +
  labs(title = "Variable Importance Plot 1") +
  theme_classic()

# plot 2
p2_rf <- importances_cs %>% 
  arrange(IncNodePurity) %>% 
  mutate(Variable = factor(Variable, levels = Variable)) %>% 
  ggplot(aes(x = IncNodePurity, y = Variable)) +
  geom_col(alpha = 0.5) +
  labs(title = "Variable Importance Plot 2") +
  theme_classic()

# side by side
p1_rf + p2_rf

# prints table of most important variables
# feature names minus year, country, cspart
rf_feature_names <- colnames(cs[,c(3:6, 8:15)])

# importance scores
rf_importance_scores <- rf_cs$importance[, 1]

# combine feature names and importance scores
rf_feature_importance <- tibble(Feature = rf_feature_names, 
                                '% Increase MSE' = rf_importance_scores)

# Print the result
print(rf_feature_importance)
```

## Comparing Models
```{r, echo = FALSE}
#| warning: false
# note: I think we are planning to cut this because
# interpretation-wise it doesn't add much that the MSE doesn't
# already cover. 

# putting together data of predicted, actual, and model type
dataplot <- data.frame(true_value = c(y[test], y[test], y[test], y[test]))
dataplot$model_type <- c(rep("Lasso", length(lasso_pred)), rep("Ridge Model", 
            length(ridge_pred)), rep("Tree Model", length(tree_pred_tuned)), 
            rep('Random Forests', length(yhat_rf)))
dataplot$predictions <- c(lasso_pred, ridge_pred, tree_pred_tuned, yhat_rf)

#plotting predicted vs actual by model type
compare <- ggplot(dataplot, aes(y = predictions, x = true_value,
                                color = model_type)) +
  geom_jitter() + geom_abline(intercept = 0, slope = 1) +
  labs(x = "Predicted Outcome", y = "Actual Outcome",
       title = 'Comparison of Model Type by Predicted vs Actual', 
       color = 'Model Type') + theme_classic() 

# function giving us R2
r2 <- function(predicted, y) {
  #find SST and SSE
  sst <- sum((y - mean(y))^2)
  sse <- sum((predicted - y)^2)
  #find R-Squared
  rsq <- 1 - sse/sst
}

# setting up values for graph
name=c("Tree","Random Forest", "LASSO","Ridge")
mse_all=c(mse, rf_mse, lasso_mse, ridge_mse)
value=c(r2(tree_pred_tuned, y[test]), r2(yhat_rf, y[test]), 
        r2(lasso_pred, y[test]), r2(ridge_pred, y[test]))

# putting into table
compare_data=tibble(name,mse_all,value)

#plotting R2
p1=ggplot(compare_data, aes(x=name, y=value))+
  geom_col()+ coord_cartesian(ylim=c(0.8,.9))+
  labs(x="Model",y="R2",title = "Comparing R2") +
  theme_classic()

# plotting MSE
p2=ggplot(compare_data, aes(x=name, y=mse_all))+
  geom_col()+ coord_cartesian(ylim=c(0.00,0.01))+
  labs(x="Model",y="MSE",title = "Comparing MSE") +
  theme_classic()
compare / (p1+p2)

# table of scores because differences may be hard to tell from graph
model_comp <- tibble('Model' = name, 
                     'MSE' = mse_all,
                     'R-Squared' = value)

# prints table
model_comp
```

# Discussion

## This section is a conclusion and discussion. This will require a summary of what you have learned about your research question along with statistical and methodological arguments supporting your conclusions. You should critique your own methods and provide suggestions for improving your analysis and future work.
### when scaled, it becomes one standard deviation change per change in the predictor

Our research question regarded predictors of civil society participation. We hypothesized that countries with similar characteristics regarding social cohesion--such as region, presence of war, social support, and civil society index--will have similar civil society participation rates. The results of our data analysis partially supported our hypothesis, with the most important predictors identified being civil society index, region, and participation in democracy, which were seen across all models, and social support, average life expectancy, and civil society repression which was found by a minority of the models run. However, social support was shown to have a negative effect on civil society participation in 2019, which we did not expect. 
Civil society index and participation in democracy were the top two predictors in all models, with region, social support, and civil society repression being identified as top predictors by some of the models. Civil society index had a positive impact on civil society participation, meaning that as elements of civil society increased so as to increase the civil society index, such as increasing the number of civil society organizations, the civil society participation would also increase. This makes sense because a higher civil society index means that there are likely more and better opportunities for people to engage in civil society, therefore increasing participation. Similarly, as participation in democracy increases, so does participation in civil society. Many civil society organizations have roles in politics, so as there is more political behavior more people likely will also engage in civil society. Further study into the relationship between civil society and participation in democracy is warranted to learn more about how these two processes interact with and influence each other. 

In terms of region, countries located in the Pacific, North Africa and the Middle East, Latin America, Southeastern Asia (identified only by tree model), and Eastern Europe and Post Soviet Union states were all associated with a decrease in civil society participation in both the tree model and regression models. This result was surprising because there were no regions associated with an increase in civil society participation, leading us to think that specific factors within a country may be more important than region overall. We were unable to find a commonality between all of these regions from our knowledge of these parts of the world or exploratory data analysis. Our choice of regression and tree methods does not work as well at explaining these differences as a clustering method may have, which could have illuminated the similarities between regions with similar civil society participation rates as well as other important factors. In terms of understanding why these regions are significant in more certain terms, further analysis, such as k-means clustering, is required. 

The other key predictors identified were civil society repression, life expectancy, and social support, which were associated with a negative effect on civil society participation. As measures to increase repression of civil societies increase, such as the decrease of influence of civil society organizations, a decrease in civil society participation is predicted. As opportunities to participate in civil society decrease or become more difficult to access, it would make sense that people would participate less in civil society as a whole. Surprisingly, an increase in social support is associated with a decrease in civil society participation. Social support is defined as whether someone feels they have someone to turn to in a difficult situation. We hypothesized that if countries were to have more social support, it meant they would have stronger civil society networks. However, the results suggest that having social support may indicate that people have strong social support networks already, decreasing the need to participate in civil society organizations. More research into this relationships is needed to fully understand the effects of social support on civil society participation. Additionally, we would like to note that there are likely more relationships between the variables than we were able to investigate, meaning social support may not truly have an overall negative effect on civil society participation as seen in the regression methods. Similarly, the relationship between this predictor, and other predictors, as well, and the outcome, may not have been linear, meaning that our two linear models may not have been the best representation. The skills of that analysis go above our abilities at this time, but are worth noting as a possibility for future study.  


## You should also discuss issues pertaining to the reliability and validity of your data and the appropriateness of the analyses should also be discussed.

Reliability is the consistency or reproducibility of the results. We did our best to make results reproducible by including set seed anytime we needed random number generation. This ensures that if someone were to replicate our project they would get the same results we did. From the provided codebooks, we can tell the methods of the data collection are consistent and can be trusted. We also made sure to document our code and thinking with comments to allow for readers to follow along with our logic and conclusions. For validity, the codebooks provided thorough descriptions of how the measures were calculated, what was included, and why. These variables should have high validity. Both the vdem group and the data from the world happiness index are wellknown, reputable sources of data, leaving us confident that their measures are as accurate as they claim to be. As for the variables we chose, there was less validity in terms of the selected variables being good measures of civil society participation. We had minimal amounts of research when determining which variables to include in our project compared to the typical researcher in this field. This means that even though the variables measured what they claimed to, they may have had less validity in the sense that the way they were used to potentially measure civil society participation was not entirely in line with the definition of civil society or the initial meaning of the variable.

The analyses we conducted were fairly appropriate. The regression models helped quantify the effects of predictors when others were held constant, giving us insight into which predictors had the most influence on the outcome. Lasso, especially, was helpful in this because it zeros out predictors that do not add enough to the model. Regression models are also easy to conceptualize and interpret, allowing both us and the reader to have a clearer picture of the relationship we are presenting. However, as discussed previously, the relationships in the data between the predictors and outcome may not be linear. This means that the results of the models may be somewhat inaccurate in their portrayal of the real relationship. With the tree models, the tuning of hyperparameters minimized flaws of the tree model. One such issue we tried to minimize was overfitting. Because trees are locally greedy (fitting the best option for that node, not the whole tree) they run the risk of overfitting based on the data. This may result in a less accurate tree overall. Similarly, because one of the biggest risks of tree models is overfitting, if the test data is especially different than the training data it will likely do poorly. This is especially relevant as we had a smaller sample, making differences in the test vs training set more impactful. Finally, tree models are costly in terms of computation/time even though they are very helpful in identifying predictors. However, we do stand by our choice of the tree models: The single decision tree gave a strong visual depiction of the most important predictors and the relationships between them and the outcome, and random forests which identifies the most important predictors and is in general more accurate in prediction ability than the single decision tree and other methods 

# You must include at least one paragraph discussing in what ways the training data you used limits any inferential conclusions or predictions for new data you can make.

The training data we used certainly limited our conclusions and predictions. Our data consisted of 187 observations, 70% of which were in the training set. While this is a majority of the countries of the world, it is still a limited sample. This greatly limited the generalizability of our results due to the smaller number of observations and because it removed any patterns that may have been present over time. 2019 was chosen due to a desire to understand the key factors in civil society participation as close to the present day, without the effects of covid, as possible. However, doing a comparison over time would give us more generalizability and predicitie ability by allowing us to see if predictors continued to be important over time or not. Additionally, 2019 did not have any recorded instances of civil wars or coups, meaning we were not able to assess the influence of these predictors on civil society participation. Going over time or picking a year that included these variables would have allowed us to better predict civil society participation by understanding the possible effects of these two predictors. In future works we would work to expand beyond 2019 or compare 2019 to a present post-covid year. Working with multiple years would have allowed us to make broader generalizations about civil society participation due to the larger amount of data we could use for training. Additionally, it would have allowed us to see patterns in civil society over time and country that we were not able to see with just the 2019 data. These data were also limited in the sense that we were not able to make generalizations or predictions about regions/countries in general because we could only predict for 2019. Another limitation of our 2019 training data is that it does not have much practical use considering 2019 is only one year and will not occur again. 

Also include a brief paragraph on what you would do differently if you were able to start over with the project.

If we were to start over with this project, we would take more time to develop our questions and analysis of the data, in addition to the training set limitations discussed in the prior paragraph. Going into the project, we did not have a strong understanding of the data we were working with. This made formulating our question difficult. Doing some data exploration to learn about the relationships between explanatory variables would have helped in formulating a question. From there, being more intentional about the data we used from the beginning would have been different, as well. In our data analysis phase, we ended up making a lot of decisions about data cleaning and data wrangling, even some about the basics of the project itself, that should have been made earlier. Choosing to focus on one year instead of multiple was a decision we do not regret, but coming to this choice at the beginning of the project would have greatly changed our data exploration and cleaning phases, as we had to approach them from a different perspective and basically redo them to do data analysis. Furthermore, learning more about the predictors themselves is a change we would make. We did not assess for covariance or any other sort of relationship between predictors. We also did not realize that some predictors we chose for our model would simply be NA for all observations, leading us to remove it later on. Additionally, the late inclusion or dropping of predictors, which proved to be tedious, would have been avoided had we taken more time to fully explore the data in the beginning. In summary, if we were to do this project again, we would do a more in-depth exploration of the data itself, think more carefully about the feasibility of our research question, then be more intentional about the data exploration and visualization we do. 