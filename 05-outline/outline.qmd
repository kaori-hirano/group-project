---
title: "Extended Outline"
author: "Group 4: Kaori Hirano, Alicia Nguyen, James Xia"
date: "07/23/23"
format: pdf
---
```{r load-packages, echo = FALSE}
library(readr)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(dplyr))
library(patchwork)
suppressPackageStartupMessages(library(glmnet)) # for ridge, LASSO
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(caret))
library(Matrix)
```

# Introduction and Data
The introduction provides motivation and context for your research. Describe your topic (citing sources) and provide a concise, clear statement of your research question and expectations.

Then identify the source of the data, when and how it was collected, the cases, a general description of relevant variables. You should describe any data wrangling that you have done, any variables you have changed from the original data set, the data tidying you had to do, and what variables you plan to include in any models you will do, and why.

we'll wnat to mention why we are using 2019 here

# Methodology
The methodology section should include visualizations and summary statistics relevant to your research question. You should also justify the choice of method(s) used to answer your research question.

# Results
Showcase how you arrived at answers to your research question using the techniques we have learned in class (and beyond, if youâ€™re feeling adventurous).

Provide only the main results from your analysis. As a reminder, the goal is not to do an exhaustive data analysis (calculate every possible statistic and perform every possible procedure for all variables). Rather, you should demonstrate that you are proficient at asking meaningful questions and answering them using data, that you are skilled in interpreting and presenting results, and that you can accomplish these tasks using R. More is not always better.
```{r choosing-base-level, echo=FALSE}
# looks for region with most observations to set as base level table(cs$region)
cs$region <- relevel(cs$region, ref = 4)
#Sub Saharan Africa, was used as the baseline because it has the most observations
```
```{r matrix-creation}
# create x and y for glmnet
set.seed(129)
x <- model.matrix(cspart ~ csrepress+v2x_partipdem+edu+corr+cs_index+social_support+
                    choices+gen+region, data = cs)[, -1]
y <- cs$cspart
```

```{r ridge-regression}
# set seed for reproducibility
set.seed(129)

# cross validation for best l
cv_r <- cv.glmnet(x[train,], y[train], alpha = 0, lambda = 10^seq(10, -2, length = 100))

# saving optimal lambda
bestlam_r <- cv_r$lambda.min

# calculating MSE
ridge_pred <- predict(cv_r, s = bestlam_r,
newx = x[test, ])
ridge_mse <- mean((ridge_pred - y[test])^2)

# fits final ridge model
ridge_mod <- glmnet(x, y, alpha = 0, lambda = bestlam_r)

# saves coefficients
coef_r <-coef(ridge_mod)
```

```{r lasso-regression}
# set seed for reproducibility
set.seed(18)
 
# cross validation for best l
cv_l <- cv.glmnet(x[train,], y[train], alpha = 1,
lambda = 10^seq(10, -2, length = 100))

# saving optimal lambda
bestlam_l <- cv_l$lambda.min

# calculating MSE
lasso_pred <- predict(cv_l, s = bestlam_l,
newx = x[test, ])
lasso_mse <- mean((lasso_pred - y[test])^2)

# fits model
lasso_mod <- glmnet(x, y, lambda = bestlam_l)

# saves coefficients
coef_l <-coef(lasso_mod)
```

```{r plot-tree, echo=FALSE}
format_tree_labels <- function(labels, levels) {
  sapply(labels, \(x) if(grepl(":", x)) clean_col(x, levels) else clean_lt(x))
} 

# replace letter positions with actual level labels
clean_col <- function(x, levels){
  # split the label into label and levels
  x <- str_split_1(x, ":")
  # make new temp objects for the two components
  var <- x[1]
  levs_ids <- x[2]
  # get levels for correct variable
  levs <- levels[[var]]
  # get level ids for *relevant* levels
  levs_ids <- str_split_1(levs_ids, "") 
  levs_ids <- sapply(levs_ids, \(x) which(letters == x))
  # cut down levs to only the required levels
  levs <- levs[levs_ids]
  # paste everything together and return (immplicitly)
  paste0(var, ": ", paste0(levs, collapse = ", "))
}

# space out labels that include only "<"
clean_lt <- function(x){
  # split on <, then recombine with spaces before and after <
  x <- str_split_1(x, "<")
  paste0(x, collapse = " < ")
}

plot_tree <- function(model){
  require(ggdendro)
  # extract necessary information from tree object so that it is ggplotable
  tree_data <- dendro_data(model)
  # create a data frame with the split *values* which dendro_data() doesn't extract
  frame <- model$frame %>%
    rownames_to_column(var = "split") %>%
    mutate(splits = as.data.frame(splits)) %>% 
    unnest(cols = c(splits)) %>% 
    filter(var != "<leaf>") %>% 
    select(cutleft)
  
  # add the splits information in, which dendro_data() misses
  tree_data$labels <- tree_data$labels %>% 
    bind_cols(frame) %>% 
    mutate(label = paste0(as.character(label), cutleft),
           label = format_tree_labels(label, attr(model, "xlevels")))
      
  ggplot(segment(tree_data)) +
    geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_text(data = label(tree_data), 
              aes(x = x, y = y, label = label), vjust = -0.5, size = 3) +
    geom_text(data = leaf_label(tree_data), 
              aes(x = x, y = y, label = label), vjust = 1.5, size = 2) +
    theme_dendro()
}
```

```{r tree-method}
#| warning: false
# sets 
set.seed(247) 
tree_train <- tree(cspart ~ . -year -country_name, cs,
    subset = train)
cv_train <- cv.tree(tree_train)
# uses 8 because that's where it levels off on the graph
prune_train <- prune.tree(tree_train, best = 8)
plot_tree(prune_train) + 
  labs(title = "Pruned Tree Plot")

tree_pred_tuned <- predict(prune_train, cs[test,],
    type = "vector")
y_test <- y[test]
# gets mse for pruned (which is same as full tree)
mse <- mean((tree_pred_tuned - y_test)^2)
```

```{r random-forests}
# sets seed
set.seed(286)

# sets training parameters
train_control <- trainControl(method="cv", number = 5)

# gets grid for mtry
tune_grid <- expand.grid(mtry = 3:11)

# does training
best_forest <- train(cspart ~ . -year -country_name, data = cs[train,], 
                     trControl = train_control, 
                     method="rf", 
                     tuneGrid = tune_grid,
                     verbose = FALSE)

# gets test for y
y_test <- y[test]

# predictions for test set with optimal mtry of 8
rf_sci <- randomForest(cspart ~ . -year -country_name, data = cs[train,], 
                       mtry = 8, importance = TRUE)
yhat_rf <- predict(rf_sci, newdata = cs[test,])

# calculates MSE
rf_mse <- mean((yhat_rf - y_test)^2)

# importances
importances_sci <- importance(rf_sci) %>% 
  as_tibble(rownames = "Variable")

# plot 1
p1_rf <- importances_sci %>% 
  arrange(`%IncMSE`) %>% 
  mutate(Variable = factor(Variable, levels = Variable)) %>% 
  ggplot(aes(x = `%IncMSE`, y = Variable)) +
  geom_col(alpha = 0.5) +
  labs(title = "Variable Importance Plot 1") +
  theme_classic()

# plot 2
p2_rf <- importances_sci %>% 
  arrange(IncNodePurity) %>% 
  mutate(Variable = factor(Variable, levels = Variable)) %>% 
  ggplot(aes(x = IncNodePurity, y = Variable)) +
  geom_col(alpha = 0.5) +
  labs(title = "Variable Importance Plot 2") +
  theme_classic()

# side by side
p1_rf + p2_rf
```

## Visualizing
```{r comparing-models}
#| warning: false
# putting together data of predicted, actual, and model type
dataplot <- data.frame(true_value = c(y[test], y[test], y[test], y[test]))
dataplot$model_type <- c(rep("Lasso", length(lasso_pred)), rep("Ridge Model", 
            length(ridge_pred)), rep("Tree Model", length(tree_pred_tuned)), rep('Random Forests', length(yhat_rf)))
dataplot$predictions <- c(lasso_pred, ridge_pred, tree_pred_tuned, yhat_rf)

#plotting predicted vs actual by model type
compare <- ggplot(dataplot, aes(x = predictions, y = true_value, color = model_type)) +
  geom_point(shape = 1) + geom_abline(intercept = 0, slope = 1) +
  labs(x = "Predicted Outcome", y = "Actual Outcome",
       title = 'Comparison of Model Type by Predicted vs Actual', 
       color = 'Model Type') + theme_classic() 

# function giving us R2
r2 <- function(predicted, y) {
  #find SST and SSE
  sst <- sum((y - mean(y))^2)
  sse <- sum((predicted - y)^2)
  #find R-Squared
  rsq <- 1 - sse/sst
}

# setting up values for graph
name=c("Tree","Random Forest", "LASSO","Ridge")
mse_all=c(mse, rf_mse, lasso_mse, ridge_mse)
value=c(r2(tree_pred_tuned, y[test]), r2(yhat_rf, y[test]), r2(lasso_pred, y[test]), r2(ridge_pred, y[test]))

# putting into table
compare_data=tibble(name,mse_all,value)

#plotting MSE
p1=ggplot(compare_data, aes(x=name, y=value))+
  geom_col()+ coord_cartesian(ylim=c(0.8,.9))+
  labs(x="Model",y="R2",title = "Comparing R2") +
  theme_classic()

# plotting R Squared
p2=ggplot(compare_data, aes(x=name, y=mse_all))+
  geom_col()+ coord_cartesian(ylim=c(0.00,0.01))+
  labs(x="Model",y="MSE",title = "Comparing MSE") +
  theme_classic()
compare / (p1+p2)
```

# Discussion
## This section is a conclusion and discussion. This will require a summary of what you have learned about your research question along with statistical and methodological arguments supporting your conclusions. You should critique your own methods and provide suggestions for improving your analysis and future work.
Our research question regarded predictors of civil society participation. We hypothesized that countries with similar characteristics regarding social cohesion--such as region, presence of war, social support, and civil society index--will have similar civil society participation rates. The results of our data analysis partially supported our hypothesis, with the most important predictors identified being civil society index, region, and participation in democracy, which were seen across all three models, and social support which was found by two of the models run, LASSO and ridge regression. However, social support was shown to have a negative effect on civil society participation in 2019, which we did not expect. 

Participation in democracy was the most important predictor identified by all three models, 


Based on these results, in future works we would work to expand beyond 2019. This severely limited the generalizability of our results due to the smaller number of observations and because it removed any patterns that may have been present over time. 


## You should also discuss issues pertaining to the reliability and validity of your data and the appropriateness of the analyses should also be discussed.
reliability is consistency/reproducibility/precision of a measure, validity is accuracy of measure (how well does it match established theories) (content validitiy is adequate coverage of the subject, criterion validity is whether it performs as expected/is a meaningful parameter, construct is does it match what it should. 

Reliability is the consistency or reproducibility of the results. We did our best to make results reproducible by including set seed anytime we needed random number generation. This ensures that if someone were to replicate our project they would get the same results we did. From the provided codebooks, we can tell the methods of the data collection are consistent and reliable. For validity, the codebooks provided thorough descriptions of how the measures were calculated, what was included, and why. These variables should have high validity. As for the variables we chose, there was less validity in terms of the selected variables being good measures of civil society participation because we had minimal amounts of research when determining which variables to include in our project. This means that even though the variables measured what they claimed to, they may have had less . 
? in terms of validity, does it refer to the way we used it or just the measurement

You must include at least one paragraph discussing in what ways the training data you used limits any inferential conclusions or predictions for new data you can make.

The training data we used certainly limited our conclusions and predictions. Our data consisted of 187 observations, 70% of which were in the training set. While this is a majority of the countries of the world, it is still a rather limited sample. Working with multiple years would have allowed us to make broader generalizations about civil society participation due to the larger amount of data we could use for training. Additionally, it would have allowed us to see patterns in civil society over time and country that we were not able to see with just the 2019 data. These data were also limited in the sense that we were not able to make generalizations or predictions about regions/countries in general because we could only predict for 2019. This does not have much practical use considering 2019 is only one year. 

Also include a brief paragraph on what you would do differently if you were able to start over with the project.

If we were to start over with this project, we would take more time to develop our questions and analysis of the data. Going into the project, we did not have a strong understanding of the data we were working with. This  made formulating our question difficult. Doing some data exploration to learn about the realtionships between explanatory variables would have helped in formulating a question. From there, being more intentional about the data we used from the beginning would have been different, as well. In our data analysis phase, we ended up making a lot of decisions about data cleaning and data wrangling, even some about the basics of the project itself, that should have been made earlier. Choosing to focus on one year instead of multiple was a decision we do not regret, but coming to this choice at the beginning of the project would have greatly changed our data exploration and cleaning phases, as we had to approach them from a different perspective and basically redo them to do data analysis. Furthermore, learning more about the predictors themselves is a change we would make. We did not assess for covariance or any other sort of relationship between predictors. We also did not realize that some predictors we chose for our model would simply be NA for all observations, leading us to remove it later on. In summary, if we were to do this project again, we would do a more in-depth exploration of the data itself, think more carefully about the feasibility of our research question, then be more intentional about the data explortation and visualization we do. 