# Analysis Experimentation

## Methods Overview
Choose 2 methods that we have talked about and use them to model the outcome 
you are interested in. Present the mathematical intuition behind the method 
and define the model in the context of your data and your research question.
If the models require hyperparameters (tuning parameters) discuss how you plan 
to go about tuning the model.

1) Lasso Regression: We are using lasso regression to choose the best predictors. 
This will allow our clustering method to be more effective by only using the 
most influential predictors. Our tuning parameters will be chosen by using cross
validation on the (insert tuning parameters here). 

2) K-means clustering: This will allow us to see the relationships between 
clusters of countries with similar characteristics. Being able to understand the 
cluster centers for each variable will provide us with information regarding 
what is important in each cluster, highlighting what the key differences 
between them are. 

## Application
Fit the two models you have presented in the previous section. If this process
involves tuning parameters, include the process of tuning the model.
```{r libraries, echo=FALSE}
# do they need to be able to see this? 
suppressPackageStartupMessages(library(glmnet)) # for ridge, LASSO
suppressPackageStartupMessages(library(tidyverse))
library(patchwork) # for plot arrangement
```

```{r data}
# import the data here
cs_full <- readRDS("/cloud/project/data/civil_society.rds")
# get only 2019
cs <- cs_full %>% subset(year == 2019)

# 70, 30 test train split
set.seed(145)
sample1 <- sample(c(TRUE, FALSE), nrow(cs), replace=TRUE, prob=c(0.7,0.3))
train <- d[sample1, ]
test <- d[!sample1, ]
```

```{r ridge-regression}
# create x and y for glmnet
x <- model.matrix(form, data = mls22_nona)[, -1]
y <- mls22_nona$base_salary_log
# do cross validation
cv_r <- cv.glmnet(x[cv,], y[cv], alpha = 0, lambda = 10^seq(10, -2, length = 100))
# saving optimal lambda
bestlam_r <- cv_r$lambda.min
# calculating MSE
ridge_pred <- predict(cv_r, s = bestlam_r,
newx = x[test, ])
ridge_mse <- mean((ridge_pred - y[test])^2)
```

```{r lasso-regression}
# set seed
set.seed(18)
# do cross validation
cv_l <- cv.glmnet(x[cv,], y[cv], alpha = 1,
lambda = 10^seq(10, -2, length = 100))
# saving optimal lambda
bestlam_l <- cv_l$lambda.min
# calculating MSE
lasso_pred <- predict(cv_l, s = bestlam_l,
newx = x[test, ])
lasso_mse <- mean((lasso_pred - y[test])^2)
# coefficients that matter
lasso_mod <- glmnet(x, y, lambda = bestlam_l)
(coef_l <-coef(lasso_mod))
```

```{r tree-method}

cv_carseats <- cv.tree(tree_carseats_train, FUN = prune.misclass)
names(cv_carseats)
cv_carseats

```

```{r plot-cv-results-baseR}
par(mfrow = c(1, 2))
plot(cv_carseats$size, cv_carseats$dev, type = "b")
plot(cv_carseats$k, cv_carseats$dev, type = "b")
```

```{r pruning}
prune_carseats <- prune.misclass(tree_carseats_train, best = 9)
plot(prune_carseats)
text(prune_carseats, pretty = 0)

plot_tree(prune_carseats)
```

```{r check-MSE}
tree_pred_tuned <- predict(prune_carseats, Carseats_test,
    type = "class")
table(tree_pred_tuned, Carseats_test$High)
(97 + 58) / 200
```

## Visualizing
Depending on the direction of your project produce at least one visualization that

predictive questions: compares model fit between the two methods
## inferential questions: compares the way in which the two models help you
## understand the relationship between your outcome and your explanatory variable of 
## interest.

# I think we do the k means cluster output graph
# then idk a linear model or something with the regression output? or one that shows
# which variables are important--correlation plot? I'm not sure...

## Discussion
In one to two paragraphs, discuss what you have learned from your initial modeling attempts.